# -*- coding: utf-8 -*-
"""Copy of complete_code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LmqfYPK98w9IoUHhQVN8eEiTFNrtfmiC
"""

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

import pandas as pd
data = pd.read_csv("/gdrive/MyDrive/group_13.csv", header=None, sep='delimiter')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import nltk
import io
import wordcloud
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from pprint import pprint
from pydoc import describe
from threading import activeCount
from venv import create
from matplotlib.style import use
import re
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud
nltk.download('stopwords')
nltk.download('wordnet')
import matplotlib.pyplot as plt #to display our wordcloud
from PIL import Image #to load our image
import numpy as np #to get the color of our image

"""### **Data Cleaning**"""

#making a separate series for date this tweet was created
date_tweeted = []
day_tweeted = []
time_tweeted = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    pos1 = cell_string.find(':"')
    pos2 = cell_string.find(' ', pos1)
    pos3 = pos2+7
    pos4 = cell_string.find(' ', pos3+1)
    day_tweeted.append(cell_string[pos1+2: pos2])
    date_tweeted.append(cell_string[pos2+1 : pos3] + ' 2022')
    time_tweeted.append(cell_string[pos3+1 : pos4])

#making a separate list for tweet id
tweet_id = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('"id":')
    end_pos = cell_string.find(',"id_str"')
    tweet_id.append(cell_string[start_pos+5:end_pos])

#making a separate list for tweet text
tweet_text = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('text')
    end_pos = cell_string.find('source', start_pos)
    tweet_text.append(cell_string[start_pos+7 : end_pos-2])

count = 0
for rows in tweet_text:
    start_pos = 0
    end_pos = rows.find('","display')
    if end_pos != -1:
        rows = rows[start_pos : end_pos]
        tweet_text[count] = rows
    count = count + 1

#making a separate list for user id
user_id = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('user":{"id":')
    end_pos = cell_string.find(',"id_str"',start_pos)
    user_id.append(cell_string[start_pos+12 : end_pos])

#making a sparate list for no. of followers
followers = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('followers_count')
    end_pos = cell_string.find(',"friends_count', start_pos)
    followers.append(cell_string[start_pos+17 : end_pos])
    

#making a separate list for no. of following
friends = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('friends_count')
    end_pos = cell_string.find(',"listed_count', start_pos)
    friends.append(cell_string[start_pos+15 : end_pos])

#making a separate list for favorites count
favorites = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('favourites_count')
    end_pos = cell_string.find(',"statuses_count', start_pos)
    favorites.append(cell_string[start_pos+18 : end_pos])

#making a separate list for statuses count
statuses = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('statuses_count')
    end_pos = cell_string.find(',"created_at', start_pos)
    statuses.append(cell_string[start_pos+16 : end_pos])

#creating a separate list for the date the account was created
created_time = []
created_day = []
created_date = []
created_year = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    pos1 = cell_string.find('created_at', cell_string.find('created_at') + 1) + 13
    pos2 = cell_string.find(' ', pos1)
    pos3 = pos2 + 8
    pos4 = cell_string.find('+0000', pos1)
    pos5 = cell_string.find('","utc_offset', pos1)
    created_day.append(cell_string[pos1 : pos2])
    created_date.append(cell_string[pos2+1 : pos3])
    created_time.append(cell_string[pos3 : pos4])
    created_year.append(cell_string[pos4+6 : pos5])

#creating a separate list for locations
locations = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('location')
    end_pos = cell_string.find(',"url', start_pos)
    locations.append(cell_string[start_pos+10 : end_pos])
    

# making a separate list for display names
display_names = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('"name":"')
    end_pos = cell_string.find('","screen_name":', start_pos)
    display_names.append(cell_string[start_pos+8 : end_pos])

# making a separate list for screen_names
screen_names = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('"screen_name":"')
    end_pos = cell_string.find('","location"', start_pos)
    screen_names.append(cell_string[start_pos+15 : end_pos])

# making a binary list to see if the tweet is a retweet 
#making a separate list to store the username who is retweeted
isRetweet = []
retweetedTo = []
for element in tweet_text:
    if element[0:2] == "RT":
        isRetweet.append(1)
        start_pos = element.find('@')
        end_pos =  element.find(':', start_pos)
        retweetedTo.append(element[start_pos+1 : end_pos])

    else:
        isRetweet.append(0)
        retweetedTo.append("null")

# making a binary list to see if the tweet is a reply
#making a separate list of the username whom the tweet is reply
isReply = []
repliedTo = []
for element in tweet_text:
    if element[0] == "@":
        isReply.append(1)
        start_pos = element.find('@')
        end_pos = element.find(" ", start_pos)
        repliedTo.append(element[start_pos+1 : end_pos])
    else:
        isReply.append(0)
        repliedTo.append('null')

# making a binar list to see if the account is verified
isVerified = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('"verified":')
    end_pos = cell_string.find(',"followers_count', start_pos)
    is_verified = cell_string[start_pos+11 : end_pos]
    if is_verified == 'false':
        isVerified.append(0)
    else:
        isVerified.append(1)

#creating a separate list for URL (if any) in the tweets 
url = []
for elements in tweet_text:
    start_pos = elements.find('https:')
    end_pos = elements.find(' ', start_pos)

    if start_pos != -1:
        url.append(elements[start_pos : end_pos])
    else:
        url.append('null')

# creating a separate list for source
source = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]   
    start_pos = cell_string.find('Twitter ', cell_string.find(',"source"'))
    end_pos = cell_string.find("\\" , start_pos, start_pos+100)
    if end_pos != -1:
        source.append(cell_string[start_pos : end_pos])
    else:
        source.append('null')

quote_count = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find(',"quote_count":')
    end_pos = cell_string.find(',"reply', start_pos)
    quote_count.append(cell_string[start_pos+15 : end_pos])

reply_count = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find(',"reply_count"')
    end_pos = cell_string.find(',"retweet', start_pos)
    reply_count.append(cell_string[start_pos+15 : end_pos])

retweet_count = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find(',"retweet_count":')
    end_pos = cell_string.find(',"favorite', start_pos)
    retweet_count.append(cell_string[start_pos+17 : end_pos])


#adding all columns to a dataframe
data = {'Date_Tweeted' : date_tweeted, 'Day_Tweeted' : day_tweeted, 'Time_Tweeted' : time_tweeted, 'Tweet_ID' : tweet_id, 
        'Tweet_Text' : tweet_text, 'User_ID' : user_id, 'Followers' : followers, 'Friends' : friends, 'Favorites' : favorites, 
        'Statuses' : statuses, 'Created_date' : created_date, 'Created_day' : created_day, 'Created_time' : created_time, 
        'Created_year' : created_year, 'Location' : locations, 'Display_names' : display_names, 
        'Screen_names' : screen_names, 'isRetweet' : isRetweet, 'retweetedTo' : retweetedTo,  'isReply' : isReply, 
        'repliedTo' : repliedTo, 'isVerified' : isVerified, 'URL' : url, 'source' : source, 'quote_count' : quote_count,
        'retweet_count' : retweet_count, 'reply_count' : reply_count}
df = pd.DataFrame(data)

import re
import pandas as pd
import numpy as np

# Load the dataset from a CSV file.
df = pd.read_csv("original_dataset.csv")

# Define a function to remove a given pattern from a given text using regular expressions.
# In this case, the pattern is any mention of a username starting with "@".
def remove_pattern(text, pattern):
    r = re.findall(pattern, text)   # Find all occurrences of the pattern in the text.
    for i in r:
        text = re.sub(i,'',text)    # Replace each occurrence of the pattern with an empty string.
    return text

# Apply the "remove_pattern" function to the "Tweet_Text" column of the dataframe to remove all username mentions.
df['Cleaned_Tweet_Text'] = np.vectorize(remove_pattern)(df['Tweet_Text'], "@[\w]*")

# Remove any characters that are not alphabets or hashtags from the tweet text, display names, and location.
df['Cleaned_Tweet_Text'] = df['Cleaned_Tweet_Text'].str.replace("[^a-zA-Z#]"," ")
df['Display_names'] = df['Display_names'].str.replace("[^a-zA-Z]"," ")
df['Location'] = df['Location'].str.replace("[^a-zA-Z]"," ")

# Remove any words that have a length of 3 or less from the tweet text, display names, and location.
df['Cleaned_Tweet_Text'] = df['Cleaned_Tweet_Text'].apply(lambda x:' '.join([w for w in x.split()if len(w)>3]))
df['Display_names'] = df['Display_names'].apply(lambda x:' '.join([w for w in x.split()if len(w)>3]))
df['Location']= df['Location'].apply(lambda x:' '.join([w for w in x.split()if len(w)>3]))

# Save the cleaned dataset to a CSV file.
df.to_csv(r"cleaned_dataset.csv", index=False)

""" **Tweet Text Cleaning**"""

#it will remove the username at the start 
def remove_pattern(text, pattern):
    r = re.findall(pattern, text)
    for i in r:
        text = re.sub(i,'',text)
    return text

df['Cleaned_Tweet_Text'] = np.vectorize(remove_pattern)(df['Tweet_Text'], "@[\w]*")

#it will remove all the characters except alphabets and hashtags from tweet text, display names and location

df['Cleaned_Tweet_Text'] = df['Cleaned_Tweet_Text'].str.replace("[^a-zA-Z#]"," ")

df['Display_names'] = df['Display_names'].str.replace("[^a-zA-Z]"," ")

df['Location'] = df['Location'].str.replace("[^a-zA-Z]"," ")


#it will remove all the words having length 3 or less from tweet text, display names and location

df['Cleaned_Tweet_Text'] = df['Cleaned_Tweet_Text'].apply(
    lambda x:' '.join([w for w in x.split()if len(w)>3]))

df['Display_names'] = df['Display_names'].apply(
    lambda x:' '.join([w for w in x.split()if len(w)>3]))

df['Location']= df['Location'].apply(
    lambda x:' '.join([w for w in x.split()if len(w)>3]))

df.to_csv(r"C:\Users\Hp\Desktop\cleaned data cvsv\Cleaned_Data.csv")

"""### **Descriptive Analysis**"""

#Total number of users, url, tweets, replies, unique users

tweet_count = df['Tweet_Text'].count()
total_retweets = df['isRetweet'].sum()
total_replies = df['isReply'].sum()
total_urls = 0
for x in df['URL']:
  if x != 'null':
    total_urls +=1
total_unique_users = df['User_ID'].nunique()


alpha = {'Total Unique Users':[total_unique_users],'Total Tweets':[tweet_count],'Total Retweets':[total_retweets],'Total Replies':[total_replies],'Total URLS':[total_urls]}
pd.DataFrame(alpha)

#making a horizontal bar graph
a1 = ['Total Unique User', 'Total Tweet','Total Retweets','Total Replies','	Total URLS' ]
b2 = [7640, 17363 , 6777,537,6804]
sns.barplot(x = b2, y = a1,).set(title='Descriptive Analysis')
    
plt.show()

#this function will make graph of the top numerical values of any variable
def plot_frequency_charts(df, feature, title, pallete):
    freq_df = pd.DataFrame()
    freq_df[feature] = df[feature]
    
    f, ax = plt.subplots(1,1, figsize=(16,4))
    total = float(len(df))
    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette=pallete)
    g.set_title("Number and percentage of {}".format(title))

    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x()+p.get_width()/2.,
                height + 3,
                '{:1.2f}%'.format(100*height/total),
                ha="center") 

    plt.title('Frequency of {} By tweets'.format(feature))
    plt.ylabel('Frequency', fontsize=12)
    plt.xlabel(title, fontsize=12)
    plt.xticks(rotation=90)
    plt.show()

plot_frequency_charts(df, 'source', 'Most Active Users', 'winter')
plot_frequency_charts(df, 'Location', 'User Locations', 'tab20')
plot_frequency_charts(df, 'Screen_names', 'User Names','ocean')


#This code will display the top 3 sources used by the user
print("The bar plot for sources: \n")
dfs =df['source'].value_counts()
dfs.head(5)

ax = df['source'].value_counts().plot(kind='bar',
                                    figsize=(14,8),
                                    title="Sources for Tweets")
ax.set_xlabel("Source")
ax.set_ylabel("Frequency")

#Top 20 Retweets and Top 20 Retweeted by screen names
print("Top 20 accounts by tweet frequency: \n")
toptweets = df.groupby('Screen_names').size().sort_values(ascending=False).reset_index()
topretweeted = df.groupby('retweetedTo').size().sort_values(ascending=False).reset_index()

print(toptweets.head(20), "\n\n", "Top 20 accounts who are retweeted the most: \n", topretweeted.head(20))

#This code will count the verified and unverified accounts
print("\n VERIFIED AND UNVERIFIED ACCOUNTS")
df['isVerified'].value_counts()

#Total number of tweets from verified accounts
verified_ac1 = df[df["isVerified"] == "1"]
verified_ac2 = df[df["isVerified"] == 1]
verify = len(verified_ac1.count(axis=1))+len(verified_ac2.count(axis=1))
print("Total number of tweets from verified accounts: ",verify )


#Total number of tweets from un-verified accounts

unverified_ac1 = df[df["isVerified"] == "0"]
unverified_ac2 = df[df["isVerified"] == 0]
uverify = len(unverified_ac1.count(axis=1))+len(unverified_ac2.count(axis=1))
print("Total number of tweets from un-verified accounts: ",uverify )

df.isVerified.value_counts().sort_values().plot(kind = 'bar', figsize=(14,8),
                                    title="Verified and Unverified accounts")
ax.set_xlabel("Verified an Unverified accounts count")
ax.set_ylabel("Frequency")
ax.set_xticklabels(["Verified","UnVerified"]) 
plt.show()


#This code will return the screen name for verified accounts
data_va = df[df["isVerified"] == 1]
print("Verified accounts have following names and followers")
data_vas = data_va[["Screen_names","Followers"]]
t_sorted = data_vas.sort_values("Followers", ascending = False)
names = t_sorted.drop_duplicates(subset=["Screen_names"], keep='first')
names = names.reset_index(drop=True)
print(names.head(50)) 





#Total number of tweets with URLS
t_url = df[df["URL"]=="null"]
print("Tweets with URLs:",df["URL"].count()-len(t_url.count(axis=1)))


#the popular Accounts:
print("Accounts By popularity")
fol = df[["Screen_names","Followers"]]
t_sorted = fol.sort_values("Followers", ascending = False)
names = t_sorted.drop_duplicates(subset=["Screen_names"], keep='first')
print(names.head(20))

time = df['Time_Tweeted']
time_13_to_14 = 0
time_14_to_15 = 0
time_15_to_16 = 0
time_16_to_17 = 0
time_17_to_18 = 0
time_18_to_19 = 0

for t in time: 
    if t[:2] == '13':
        time_13_to_14 = time_13_to_14 + 1
    elif t[:2] == '14':
        time_14_to_15 = time_14_to_15 + 1
    elif t[:2] == '15':
        time_15_to_16 = time_15_to_16 + 1
    elif t[:2] == '16':
        time_16_to_17 = time_16_to_17 + 1
    elif t[:2] == '17':
        time_17_to_18 = time_17_to_18 + 1
    elif t[:2] == '18':
        time_18_to_19 = time_18_to_19 + 1

print('Tweets by hour:')
print('1:23PM to 2PM: ', time_13_to_14)
print('2PM to 3PM: ', time_14_to_15)
print('3PM to 4PM: ', time_15_to_16)
print('4PM to 5PM: ', time_16_to_17)
print('5PM to 6PM: ', time_17_to_18)
print('6PM to 6:29PM: ', time_18_to_19)


date = pd.to_datetime(df['Time_Tweeted'], format='%H:%M:%S')

fig, ax = plt.subplots(figsize =(10, 7))
plt.hist(date, edgecolor='black')
#ax.set_xticklabels(["1:23PM to 2PM", "2PM to 3PM", "3PM to 4PM", "4PM to 5PM", "5PM to 6PM", "6PM to 6:29PM"])

"""### **Sentiment Analysis**"""

df['Cleaned_Tweet_Text'] = df['Cleaned_Tweet_Text'].astype(str)

from nltk.corpus import stopwords
stop = set(stopwords.words('english'))

#this code will remove all the stopwords
df['tidy_tweets'] = df['Cleaned_Tweet_Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

#this code will generate wordcloud
words = ' '.join([text for text in df['tidy_tweets']])
wordcloud = WordCloud(width = 800, height = 500,  background_color="rgba(255, 255, 255, 0)", mode="RGBA",random_state = 21, max_font_size = 110).generate(words)

plt.figure(figsize = (10,7))
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.show()

#it will analyze tweets as positive or negative
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sent = SentimentIntensityAnalyzer()

df['scores'] = df['tidy_tweets'].apply(lambda x: sent.polarity_scores(x))

df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])

#wordcloud for positive worded tweets 

positive = ' '.join([text for text in df['tidy_tweets'][df['compound'] >= 0.7]])

wordcloud = WordCloud(width = 800, height = 500, background_color="rgba(255, 255, 255, 0)", mode="RGBA",random_state = 21, max_font_size = 110).generate(positive)

plt.figure(figsize = (10,7))
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.show()


#negative worded tweets
negative = ' '.join([text for text in df['tidy_tweets'][df['compound'] <= -0.7 ]])

wordcloud = WordCloud(width = 800, height = 500,background_color="rgba(255, 255, 255, 0)", mode="RGBA", random_state = 21, max_font_size = 110).generate(negative)

plt.figure(figsize = (10,7))
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.show()

#total positive negative and neutral tweets
positive = 0
negative = 0
neutral = 0

for i in df['compound']:
    if i > 0 :
        positive +=1
    elif i < 0:
        negative  +=1
    else:
        neutral +=1
print(positive, negative, neutral)

total_count = {'Positive words': [4938] , 'Negative words': [1147] , 'Neutral' : [11278] }
sent = pd.DataFrame(total_count)
sent

a1 = ['Positive', 'Negative','Neutral' ]
b2 = [4938, 1147 , 11278]
sns.barplot(x = a1,
            y = b2,).set(title='Sentiment Analysis')
plt.show()

"""### **Hashtag Analysis**"""

data = pd.read_csv("/gdrive/MyDrive/group_13.csv", header=None, sep='delimiter')
hashtags = []
cell_string = ''
trending = []

for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    trending = re.findall('#[A-Za-z]+[A-Za-z0-9-_]+', cell_string)
    hashtags = hashtags + trending

print('Total Hashtags: ', len(hashtags))
arr_hashtags = np.array(hashtags)
print('Unique Hashtags: ', len(np.unique(arr_hashtags)))

df = pd.DataFrame(hashtags)
df.value_counts()[0:20].plot(kind = 'barh', title = 'Top 20 trending on 23 Feb 2022', xlabel = 'trending')
plt.show()
plt.savefig('Trending_BarChat.png')

# Making a worldcloud of hashtags
print('World Cloud of Most Trending on 23 Feb 2022')
text = str(hashtags)
word_cloud = WordCloud(collocations = False, background_color = 'white', width = 400, height = 400).generate(text)
word_cloud.to_file('Trending_WordCloud.png')
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""### **Location Analysis**"""

#Forming a word cloud of locations
data = pd.read_csv("/gdrive/MyDrive/group_13.csv", header=None, sep='delimiter')
locations = []
for rows in range(len(data)):
    cell_string = data.iloc[rows][0]
    start_pos = cell_string.find('location')
    end_pos = cell_string.find(',"url', start_pos)
    location = cell_string[start_pos+10 : end_pos]
    location = location.replace('\\', '')
    location = location.replace(',', '')
    location = location.replace('"', '')
    location = re.sub(r'[0-9]', ' ', location)
    location = re.sub(r'\b\w{1,3}\b', '', location)
    start_pos = 0
    end_pos = location.find(' ')
    if end_pos != -1:
        location = location[start_pos : end_pos]
    location = location.lower()
    locations.append(location)
  
locations.remove('null')
text_locations = str(locations)
    
print('World Cloud of Locations on 23 Feb 2022')
word_cloud = WordCloud(collocations = False, background_color = 'white', width = 400, height = 400).generate(text_locations)
word_cloud.to_file('Locations_WordCloud.png')
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()